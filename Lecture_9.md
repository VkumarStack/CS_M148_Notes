# Lecture 9
## Motivation
- Recall that, with logistic regression, the *decision boundary* is defined where the probability of being in class 0 and class 1 are equal
    - The boundary with logistic regression is a *hyperplane*
- Decision boundaries can be generalized to higher order polynomial terms, but even these may not be powerful enough to handle complex decision boundaries 
## Decision Trees
- Decision trees partition the data into *different regions* and make predictions
- ![Decision Tree](./Images/Decision_Tree.png)
    - The final outcome is based on a series of comparisons of the values of predictors against threshold values
    - The *root* and *internal* nodes of the tree represent attribute testing
    - *Branching* is determined by attribute value
    - *Terminal leafs* represent class assignments
- Every flow chart tree corresponds to a partition of the feature space by **axis-aligned lines or (hyper) planes**
    - ![Decision Tree Partition](./Images/Decision_Tree_Partition.png)
    - Each comparison and branching results in splitting a region in the feature space on a *single feature*
## Learning the Model
- Learning a decision tree for binary classification involves producing an *optimal* partition of the feature space with axis-aligned linear boundaries
    - Each region is predicted to have a class label based on the *largest class* of training points in that region when performing prediction
- A reasonable model can be found using a *greedy algorithm*:
    - Start with an empty decision tree
    - Choose the optimal predictor (feature) on which to split and choose the optimal threshold value for splitting
        - For numerical variables, a threshold can be used to partition the data
        - For categorical variables, the possible value can be used to partition the data
    - Recurse on each new node until the stopping condition is met
        - The stopping condition should be defined
### Splitting Criteria
- The regions in the feature space should grow progressively more "pure" with the number of splits - each region should specialize towards a single class
    - Example: 
        - ![](./Images/Decision_Tree_Splitting.png)
        - The left attribute (ages) is better for splitting as it produces more *pure* regions
- There are various ways to define the "purity" of a region:
    - **Classification Error**: Consider  *J* predictors and *K* classes, and the $j^{th}$ predictor is selected, splitting a region containing $N$ training points along the threshold $t_j$
        - The **classification error** made by each newly created region $R_1$, $R_2$ as: $Error(i | j, t_j) = 1 - \max_k p(k|R_i)$
            -  $p(k|R_i)$ is the percentage of data points belonging to the $k^{th}$ class given that they are in region $R_i$
            - This is essentially majority voting - the more data points of the same class in a given region, the lower the error
        - For each region, the classification error can be computed and then the *average classification error* can be found
        - The goal is find the predictor $j$ and threshold $t_j$ that minimizes the *average classification error* over the regions, weighted by the population of the regions:
            - $\min_{j, t_j} [ \frac{N_1}{N} Error(1| j, t_j) + \frac{N_2}{N} Error(2| j, t_j)] $
        - Example:
            - ![Classification Error](./Images/Classification_Error.png)
    - **Gini Index**: Consider  *J* predictors and *K* classes, and the $j^{th}$ predictor is selected, splitting a region containing $N$ training points along the threshold $t_j$
        - The **Gini Index** measures the purity of each region by the term: $Gini(i | i ,t_j) = 1 - \sum_k p(k | R_i)^2$
            - Without the quadratic term, the summation would yield *1*, so the index would always yield zero
        - The goal is find the predictor $j$ and threshold $t_j$ that minimizes the *average Gini index* over the regions, weighted by the population of the regions:
            - $\min_{j, t_j} [ \frac{N_1}{N} Gini(1| j, t_j) + \frac{N_2}{N} Gini(2| j, t_j)] $
        - ![Gini Index](./Images/Gini_Index.png)
    - **Entropy** The strength of a signal in a particular region can be quantified by computing its *entropy*
        - $H(X)= - \sum p(x) log_2 p(x)$
        - The entropy is zero when the region is very pure (all in one class) and is high (one) when values are less pure (unpredictable) - e.g. when the probability is 0.5
        - The entropy can also be averaged:
            - $\min_{j, t_j} [ \frac{N_1}{N} Entropy(1| j, t_j) + \frac{N_2}{N} Entropy(2| j, t_j)]$
        - Example: 
            - ![Entropy](./Images/Entropy.png)
- Splitting should also not yield empty regions
### Stopping Criteria
- If the decision tree learning algorithm is not terminated manually, the tree will continue to grow until each region defined by the model possibly contains exactly *one training point*
    - This will result in a 100% training accuracy, which is a clear sign of overfitting (it will not generalize well)
- Stopping based on depth:
    - Small Depth:
        - There is *high bias*, as the model is unable to capture the nonlinear boundary separating two classes 
        - However, there is a *low variance*, as the model is robust to slight pertubations in the training data
    - High Depth:
        - There is *low bias*, as the model is sufficiently complex enough to classify points on the boundary (e.g. by zig-zagging)
        - There is, however, a *high variance*, as the model is sensitive to pertubations in the training data
- Common Conditions:
    - Do not split a region if all instances in the region belong to the *same class*
    - Do not split a region if the number of instances in the region will fall below a pre-defined threshold
    - Do not split a region if the total number of leaves in the tree will exceed a pre-defined threshold
- Cross-validation can be used to find appropriate thresholds
- Another common stopping condition is to determine the gain in *purity* from splitting a region $R$ into $R_1$ and $R_2$
    - $Gain(R) = \Delta(R) = m(R) - \frac{N_1}{N}m(R_1) - \frac{N_2}{N}m(R_2)$
        - $m$ is a metric such as Classification Error, Gini Index, or entropy
    - If the gain is less than some pre-defined threshold, then stopping should occur
-  An issue with *pre-specified* stopping conditions is that a tree may *stop too early* or *stop too late* - there is not enough control
    - An alternative can be to allow a tree to grow freely and then **prune** it according to *post-specified* stopping conditions
    - ![Pruning 1](./Images/Pruning_1.png)
        - ![Pruning 2](./Images/Pruning_2.png)
        - The tree is let grown freely until all regions are *pure* (e.g. zero Gini Index or Classification Error or Entropy), but then it is pruned to account for the obvious overfit
        - The difference in this case is that the stopping criterion (e.g. a small number of values in a region) is applied *after* the tree is fully grown rather than stopping the tree from growing prematurely
    - ![Pruning Fit](./Images/Pruning_Fit.png)
        - This approach can often produce better models, but may require more computation power in order to allow for the tree to grow fully
## Regression Trees
- Decision trees can be extended to perform regression with some modifications
    - The value based on a decision tree should be the *average* of a region 
- The splitting criteria can now no longer be based on purity, since there are no classes
    - Instead, the splitting criteria should be based on a different measure such as MSE
    - $\min{\frac{N_1}{N}MSE(R_1) + \frac{N_2}{N}MSE(R_2)}$
        - $MSE = \frac{\sum_i (y_i - \bar{y})^2}{n}$ since the predicted value for a region is just its average value
- Just like with the classification case, the stopping conditions can be based on an accuracy gain:
    - $Gain(R) = \Delta(R) = MSE(R) - \frac{N_1}{N}MSE(R_1) + \frac{N_2}{N}MSE(R_2)$
- Example:
    - ![Regression Tree](./Images/Regression_Tree.png)
    - The cutoffs here are at $x=3.5, x=6.8, x=7.5$
## Random Forest
- A related approach is to **ensemble** decision trees, creating a *random forest*
    - A single tree may not have high accuracy or may be prone to overfitting, but by using *multiple trees* this can be mitigated
- Forests can be constructed via **bootstrap aggregating**, where the original training data is downsampled into multiple datasets (constructed with replacement) and multiple classifiers can be build from these multiple datasets
    - A decision can be made from majority voting (for classification) or averaging the result of the different trees
    - The different trees often sample a *subset* of the features of the dataset (not necessarily all of the features)
- Each classifier produces a prediction $f_i(x)$, which may have low bias but also *high variance*
    - The error can be reduced if the average of multiple classifiers is used since the variance would be reduced: $var(\frac{\sum_i f_i(x)}{t}) = var(f_i(x))/t$
- Strengths and Weaknesses:
    - Random forest has very good accuracy for classification tasks and can handle large-scale datasets
    - However, random forest is not necessarily good prediction tasks (e.g. regression) nor can it be interpreted well